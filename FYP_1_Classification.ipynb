{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import matplotlib.pyplot as mp\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import datetime\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.tree import export_graphviz\n",
    "import graphviz\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(filename, comma):\n",
    "    \n",
    "    if comma:\n",
    "        df = pd.read_csv(filename)\n",
    "    else:\n",
    "        df = pd.read_csv(filename, sep=';')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_data('cleanedData.csv', True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = get_data('cleanedData.csv', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=get_data('trainSet.csv', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=get_data('testSet.csv', True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=train.pop('Booked').values\n",
    "X_train=train.values\n",
    "\n",
    "y_test=test.pop('Booked').values\n",
    "X_test=test.values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale the data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler=MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train=scaler.transform(X_train)\n",
    "X_test=scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Minority in training set : %d\" % (Counter(y_train)[1]))\n",
    "print(\"Minority in test set : %d\" % (Counter(y_test)[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models= {}\n",
    "models['dtree'] =  DecisionTreeClassifier(criterion='entropy')\n",
    "models['logistic'] = LogisticRegression(max_iter=10000, \n",
    "                                        #solver = 'sag', \n",
    "                                        random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reps=100\n",
    "reps=2\n",
    "for model in models:\n",
    "    xv=[]\n",
    "    xv_auc=[]\n",
    "    for i in range(reps):\n",
    "        kf = KFold(n_splits=10, shuffle = True) \n",
    "        scores = cross_val_score(models[model], X_train, y_train, cv=kf)\n",
    "        xv.append(scores.mean())\n",
    "        scores_lg = cross_val_score(models[model], X_train, y_train, scoring='roc_auc',cv=kf)\n",
    "        xv_auc.append(scores_lg.mean())\n",
    "    avg_acc=reduce(lambda a, b: a + b, xv) / len(xv)\n",
    "    avg_auc=reduce(lambda a, b: a + b, xv_auc) / len(xv_auc)\n",
    "    print(\"{:22} Avg. Accuracy: {:.2f} Avg. AUC {:.2f}\".format(type(models[model]).__name__,avg_acc, avg_auc)) \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing for Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "def tp(y_true, y_pred): return confusion_matrix(y_true, y_pred)[0, 0]\n",
    "def tn(y_true, y_pred): return confusion_matrix(y_true, y_pred)[1, 1]\n",
    "def fp(y_true, y_pred): return confusion_matrix(y_true, y_pred)[1, 0]\n",
    "def fn(y_true, y_pred): return confusion_matrix(y_true, y_pred)[0, 1]\n",
    "scoring = {'tp' : make_scorer(tp), 'tn' : make_scorer(tn),\n",
    "           'fp' : make_scorer(fp), 'fn' : make_scorer(fn)}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_cv = {}\n",
    "folds=10\n",
    "print(\"Minority in data set : %d\" % Counter(y_train)[1])\n",
    "for m in models:\n",
    "    cv_results = cross_validate(models[m], X_train, y_train, cv= folds,scoring=scoring, return_train_score=False, \n",
    "                                    verbose = 0, n_jobs = -1)\n",
    "    n_tot = cv_results['test_tn'].sum() + cv_results['test_fn'].sum()\n",
    "    acc = (cv_results['test_tp'].sum() + cv_results['test_tn'].sum())/len(y_train)\n",
    "    bias_cv[m] = n_tot\n",
    "  \n",
    "    print(\"{} x CV {:22} No. of bags in dataset: {:d} Pred no. bags: {:d} Acc: {:.2f}\".format(folds, \n",
    "                                                                  type(models[m]).__name__, \n",
    "                                                                  Counter(y_train)[1], \n",
    "                                                                  n_tot,acc)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Logistic regression is accurate but very bias predicitng only 27 bags being bought"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rectifying Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create pipeline w/ SMOTE and a classifier\n",
    "steps_all = {}\n",
    "smt=SMOTE(random_state=0)\n",
    "steps_all['dtree']=[('smt', smt), \n",
    "                    ('dtree', models['dtree'])]\n",
    "steps_all['logistic']=[('smt', smt), \n",
    "                      ('LR', models['logistic'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to check the effect of smote\n",
    "\n",
    "for m in steps_all:\n",
    "    pipeline=Pipeline(steps_all[m])\n",
    "    xv_acc = []\n",
    "    xv_auc=[]\n",
    "    bias_l=[]\n",
    "    \n",
    "    #running the cross validation multiple times\n",
    "    for i in range(reps):\n",
    "        kf = KFold(n_splits=10,shuffle = True)\n",
    "        \n",
    "        #testing the bias\n",
    "        cv_results = cross_validate(pipeline, X_train, y_train, cv= kf,scoring=scoring, return_train_score=False, \n",
    "                                    verbose = 0, n_jobs = -1)\n",
    "        n_tot = cv_results['test_tn'].sum() + cv_results['test_fn'].sum()\n",
    "        bias_l.append(n_tot)\n",
    "       \n",
    "        scores_lg = cross_val_score(pipeline, X_train, y_train,cv=kf)\n",
    "        xv_acc.append(scores_lg.mean())\n",
    "        scores_lg = cross_val_score(pipeline, X_train, y_train, scoring='roc_auc',cv=kf)\n",
    "        xv_auc.append(scores_lg.mean())\n",
    "    avg_bags=reduce(lambda a, b: a + b, bias_l) / len(bias_l)\n",
    "    avg_acc=reduce(lambda a, b: a + b, xv_acc) / len(xv_acc)\n",
    "    avg_auc=reduce(lambda a, b: a + b, xv_auc) / len(xv_auc)\n",
    "    \n",
    "    print(\"{:22} Avg. Pred bag is booked: {:.2f} Avg. Accuracy: {:.2f} Avg. AUC: {:.2f}\".format(type(models[m]).__name__, \n",
    "                                                              avg_bags,avg_acc, avg_auc))\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif, SelectKBest\n",
    "from sklearn.feature_selection import RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xv=[]\n",
    "xv_auc=[]\n",
    "for i in range(reps):\n",
    "    kf = KFold(n_splits=10, shuffle = True) \n",
    "    scores = cross_val_score(Pipeline(steps_all['dtree']), X_train, y_train, cv=kf)\n",
    "    xv.append(scores.mean())\n",
    "    scores_lg = cross_val_score(Pipeline(steps_all['dtree']), X_train, y_train, scoring='roc_auc',cv=kf)\n",
    "    xv_auc.append(scores_lg.mean())\n",
    "avg_acc=reduce(lambda a, b: a + b, xv) / len(xv)\n",
    "avg_auc=reduce(lambda a, b: a + b, xv_auc) / len(xv_auc)\n",
    "print(\"{:22} Avg. Accuracy: {:.2f} Avg AUC: {:.2f}\".format(type(models['dtree']).__name__,avg_acc, avg_auc)) \n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#get the mutual information of each feature\n",
    "i_scores = mutual_info_classif(X_train,y_train,random_state=42)\n",
    "i_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi=dict()\n",
    "for i,j in zip(df.columns,i_scores):\n",
    "    mi[i]=j\n",
    "df_feat=pd.DataFrame.from_dict(mi,orient='index',columns=['I-Gain'])\n",
    "\n",
    "#only considering the top 50 features\n",
    "df_feat=df_feat.sort_values(by=['I-Gain'],ascending=False)[0:50]\n",
    "df_feat.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedStratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluates each feature subset\n",
    "def evaluate_model(model, X, y, scoring):\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    scores = cross_val_score(model, X, y, scoring=scoring, cv=cv)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculates the mean of the scores\n",
    "def getMean(scores):\n",
    "    sum=0\n",
    "    N=len(scores)\n",
    "    \n",
    "    for i in scores:\n",
    "        sum+=i\n",
    "    \n",
    "    avg=sum/N\n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_scores = []\n",
    "acc_scores=[]\n",
    "for kk in range(1, df_feat.shape[0]+1):\n",
    "    #increases the number of features at each iteration\n",
    "    fs = SelectKBest(mutual_info_classif, \n",
    "                           k=kk)\n",
    "  \n",
    "    pipeline_feat=Pipeline(steps=[('anova', fs), \n",
    "                                ('dtree', Pipeline(steps_all['dtree']))] )\n",
    "    #calculate the auc score for each subset\n",
    "    auc=evaluate_model(pipeline_feat, X_train, y_train, 'roc_auc')\n",
    "    auc_scores.append(auc)\n",
    "                           \n",
    "    #acc=evaluate_model(pipeline_feat, X_train, y_train, 'accuracy')\n",
    "    #acc_scores.append(acc)\n",
    "\n",
    "df_feat['AUC']=[getMean(auc_scores[i]) for i in range(len(auc_scores))]\n",
    "#df_feat['Accuracy']=[getMean(acc_scores[i]) for i in range(len(acc_scores))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gets the index of the highest AUC score\n",
    "finalFeat=df_feat['AUC'][df_feat['AUC']==df_feat['AUC'].max()].index[0]\n",
    "finalFeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gets the names subset of features that produced the highest auc scores\n",
    "select_feat=list(df_feat[:finalFeat].index)\n",
    "select_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_feat=[df.columns.get_loc(c) for c in select_feat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_aft=X_train[:, auc_feat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_feat\n",
    "'''\n",
    "result:\n",
    "select_feat=['MaxExternalBookingID','TreatmentProductSequence','ProductID',\n",
    "        'TotalFare','OriginalPrice','SegmentDestinationLocationCode_BIS','FareClass_M']\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xv=[]\n",
    "xv_auc=[]\n",
    "for i in range(10):\n",
    "    kf = KFold(n_splits=10, shuffle = True) \n",
    "    scores = cross_val_score(Pipeline(steps_all['dtree']), X_aft, y_train, cv=kf)\n",
    "    xv.append(scores.mean())\n",
    "    scores_lg = cross_val_score(Pipeline(steps_all['dtree']), X_aft, y_train, scoring='roc_auc',cv=kf)\n",
    "    xv_auc.append(scores_lg.mean())\n",
    "avg_acc=reduce(lambda a, b: a + b, xv) / len(xv)\n",
    "avg_auc=reduce(lambda a, b: a + b, xv_auc) / len(xv_auc)\n",
    "print(\"{:22} Avg. Accuracy: {:.2f} Avg AUC: {:.2f}\".format(type(models['dtree']).__name__,avg_acc, avg_auc)) \n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AUC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=Pipeline(steps_all['logistic'])\n",
    "clf.fit(X_aft, y_train)\n",
    "X_test_aft=X_test[:,auc_feat]\n",
    "probs_lr=clf.predict(X_test_aft)\n",
    "auc_lr = roc_auc_score(y_test, probs_lr)\n",
    "print(round(auc_lr,2)) #.64\n",
    "\n",
    "fpr_lr, tpr_lr, thresholds_lr = roc_curve(y_test, probs_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=Pipeline(steps_all['dtree'])\n",
    "clf.fit(X_aft, y_train)\n",
    "X_test_aft=X_test[:,auc_feat]\n",
    "probs_dt=clf.predict(X_test_aft)\n",
    "auc_dt = roc_auc_score(y_test, probs_dt)\n",
    "print(round(auc_dt,2)) #.71\n",
    "\n",
    "fpr_dt, tpr_dt, thresholds_dt = roc_curve(y_test, probs_dt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the roc curve for decision tree & logistic regression\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.plot(fpr_lr, tpr_lr, label=f'AUC (Logistic Regression) = {auc_lr:.2f}')\n",
    "plt.plot(fpr_dt, tpr_dt, label=f'AUC (Decision Tree) = {auc_dt:.2f}')\n",
    "plt.plot([0, 1], [0, 1], color='blue', linestyle='--', label='Baseline')\n",
    "plt.title('ROC Curve', size=20)\n",
    "plt.xlabel('False Positive Rate', size=14)\n",
    "plt.ylabel('True Positive Rate', size=14)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Price Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=Pipeline(steps_all['dtree'])\n",
    "\n",
    "#only using one price feature\n",
    "select_feat.remove('TotalFare')\n",
    "originalPriceIndex=select_feat.index('OriginalPrice')\n",
    "\n",
    "#using the features chosen in feature selection\n",
    "auc_feat=[df.columns.get_loc(c) for c in select_feat]\n",
    "clf.fit(X_train[:,auc_feat], y_train)\n",
    "X_test_aft=X_test[:,auc_feat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfOriginalPriceIndex=df.columns.get_loc(\"OriginalPrice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all prices offered in the test set\n",
    "all_prices=pd.DataFrame(data=scaler.inverse_transform(X_test))[dfOriginalPriceIndex]\n",
    "#get the min and max price in the test set\n",
    "maxPrice=pd.DataFrame(data=scaler.inverse_transform(X_train))[dfOriginalPriceIndex].max()\n",
    "minPrice=pd.DataFrame(data=scaler.inverse_transform(X_train))[dfOriginalPriceIndex].min()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale each price point inputted using min max scaler\n",
    "def scalePrice(X):\n",
    "    y = (X - minPrice) / (maxPrice - minPrice)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inverse transform of scalePrice\n",
    "def deScale(y):\n",
    "    x=(y*(maxPrice-minPrice)) + minPrice  \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices = [*range(25,51)]\n",
    "scaleprices = scalePrice(np.array(prices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#outputs the recommended price for each test instance\n",
    "def choosePrice(current_instance):\n",
    "    chosenPrice=25\n",
    "    highestPred =0\n",
    "\n",
    "    XX = np.zeros((len(prices),current_instance.shape[1]));\n",
    "    XX[0:len(prices),:] = current_instance\n",
    "    XX[0:len(prices),originalPriceIndex] = scaleprices\n",
    "    prediction=clf.predict_proba(XX)[:,1]\n",
    "    indx = np.argmax(prediction)\n",
    "    \n",
    "    \n",
    "    return XX[indx,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newArray=np.zeros((X_test.shape[0],len(select_feat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(X_test.shape[0]):\n",
    "    newArray[i]=choosePrice(X_test_aft[i].reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store all recommended prices outputted by the pricing model\n",
    "predictedPrice=[] \n",
    "for i in range(X_test.shape[0]):\n",
    "    price= deScale(newArray[i][originalPriceIndex])\n",
    "   # print(price)\n",
    "    predictedPrice.append(price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predPrice=pd.Series(predictedPrice,name='predPrice')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation - Revenue Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Set Revenue\n",
    "Get the actual revenue generated by the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testResults=pd.DataFrame()\n",
    "testResults['TestPrices']=all_prices\n",
    "testResults['y_test']=y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_booked=testResults[testResults.y_test==1]#.shape\n",
    "testRevenue=y_booked.TestPrices.sum()\n",
    "testRevenue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicted Revenue from Pricing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accepts = np.zeros(np.size(prices,0))\n",
    "counts = np.zeros(np.size(prices,0))\n",
    "all_prices = round(all_prices)\n",
    "i=0;\n",
    "for p in prices:\n",
    "    counts[i] = np.sum(all_prices==p)\n",
    "    accepts[i]=np.sum(y_test[all_prices==p])\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoothcounts(counts,accepts):\n",
    "    #the number of test instances\n",
    "    N = np.sum(counts,0)\n",
    "    rejects = counts - accepts\n",
    "    \n",
    "    #for each price count the number of acceptances for that price & all prices lower\n",
    "    cuma = np.flip(np.cumsum(np.flip(accepts)))\n",
    "    cumr = np.cumsum(rejects)\n",
    "    \n",
    "    cumm = cuma + cumr\n",
    "    \n",
    "    #the number of accepts asociated with a price / number of times it was offered\n",
    "    rawprob = accepts/(counts+(counts==0))\n",
    "    print(\"rawprob is\",rawprob)  \n",
    "    \n",
    "    alpha = rawprob\n",
    "   \n",
    "   \n",
    "    #taking into account the monotonicity in a customer's willingness to pay\n",
    "    prob2 = (cuma + (N-cumm)*alpha)/N   \n",
    "    print(\"prob2 is\",prob2)\n",
    "    return rawprob,prob2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawprob,prob2=smoothcounts(counts,accepts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the purchasing probability for each price point\n",
    "priceProbs = dict()\n",
    "i=0\n",
    "for p in range(25,51):\n",
    "    priceProbs[p] = prob2[i]\n",
    "    i=i+1\n",
    "priceProbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the revenue generated by the pricing model\n",
    "revenue = 0\n",
    "for p in predPrice:\n",
    "    revenue = revenue + p*priceProbs[p]\n",
    "revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
